{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1630fcd6-b23d-4015-b227-8f0c4b27ea34",
   "metadata": {},
   "source": [
    "<h2> Running Bayes with a Normal prior</h2>\n",
    "Now I make another set of Bauesian tests with a normal prior. WHy don't I do this in the other notebook? Because the tests kept crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fdf11b-b395-4128-8a94-c0b1b94dd4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# Importing Things #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "from scipy import stats\n",
    "from scipy.stats import gennorm\n",
    "from scipy.stats import gamma\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "badbb148-1860-4fbf-89ef-37cab9e623e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gGaussMean = 3\n",
    "gGaussSpread = 2.5\n",
    "gGaussBeta = 3\n",
    "prior_mean = 24\n",
    "numTests=40\n",
    "lift=1.03\n",
    "resultFile = r\"C:\\Users\\efree\\BayesTests_Normal_v0.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557154c9-b0ef-42a9-8a12-02fbc1e4a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N=48000, R=0.60, M=gGaussMean, S=gGaussSpread, B=gGaussBeta,lift=1.00):\n",
    "    first_col = np.random.choice(['T', 'C'], size=N)\n",
    "    second_col = np.zeros(N)\n",
    "    non_zero_count = int((1 - R) * N)\n",
    "    non_zero_indices = np.random.choice(N, size=non_zero_count, replace=False)\n",
    "    random_values = gennorm.rvs(beta=B, loc=M, scale=S, size=non_zero_count)\n",
    "    second_col[non_zero_indices] = np.exp(random_values)\n",
    "    df = pd.DataFrame({\n",
    "        'TestControl': first_col,\n",
    "        'Revenue': second_col\n",
    "    })\n",
    "    df['Revenue']=df.apply(lambda x: x['Revenue']*lift if x['TestControl']=='T' else x['Revenue'],axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1594bda2-9fe4-45d9-890b-d0bf77ddf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurdle_normal_test(data: pd.DataFrame, samples: int = 3000, tune: int = 1000, random_seed: int = 42):\n",
    "    # Split the data into test and control groups\n",
    "    Control_data = data[data['TestControl'] == 'C']['Revenue'].values\n",
    "    Test_data = data[data['TestControl'] == 'T']['Revenue'].values\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Priors for group means and standard deviations\n",
    "        mu_control = pm.Normal('mu_control', mu=prior_mean, sigma=10)\n",
    "        sigma_control = pm.HalfNormal('sigma_control', sigma=10)\n",
    "\n",
    "        mu_test = pm.Normal('mu_test', mu=prior_mean, sigma=10)\n",
    "        sigma_test = pm.HalfNormal('sigma_test', sigma=10)\n",
    "\n",
    "        # Likelihoods for observed data\n",
    "        control_obs = pm.Normal('control_obs', mu=mu_control, sigma=sigma_control, observed=Control_data)\n",
    "        test_obs = pm.Normal('test_obs', mu=mu_test, sigma=sigma_test, observed=Test_data)\n",
    "\n",
    "        # Deterministic variable for the difference in means\n",
    "        delta = pm.Deterministic('delta', mu_test - mu_control)\n",
    "        revenue_T = pm.Deterministic('revenue_T',mu_test)\n",
    "        revenue_C = pm.Deterministic('revenue_C', mu_control)\n",
    "\n",
    "        # Sampling\n",
    "        trace = pm.sample(samples, tune=tune, random_seed=random_seed, return_inferencedata=True, chains=2, target_accept=0.9)\n",
    "\n",
    "    # Calculate the probability that test mean > control mean\n",
    "    delta_samples = trace.posterior['delta'].values.flatten()\n",
    "    percent_chance_T_better = (delta_samples > 0).mean()\n",
    "\n",
    "    return percent_chance_T_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6eecc6-c2a4-44f1-8bc0-db06e0bf99b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 179 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 173 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 181 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 168 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 205 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 160 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 164 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 180 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 260 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 181 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 236 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 186 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 179 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 183 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 180 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 189 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 274 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 175 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 177 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 280 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 176 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 201 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 190 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 179 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 181 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 183 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 180 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 185 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 183 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 200 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 176 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 220 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 184 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 262 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 184 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 329 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 197 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_control, sigma_control, mu_test, sigma_test]\n",
      "Sampling 2 chains for 1_000 tune and 3_000 draw iterations (2_000 + 6_000 draws total) took 206 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "for i in range(numTests):\n",
    "    data1 = generate_dataset(lift=lift)\n",
    "    t_mean = data1[data1['TestControl']=='T']['Revenue'].mean()\n",
    "    c_mean = data1[data1['TestControl']=='C']['Revenue'].mean()\n",
    "    t_statistic, p_value = stats.ttest_ind(data1[data1['TestControl']=='T']['Revenue'].values, data1[data1['TestControl']=='C']['Revenue'].values)\n",
    "    try:\n",
    "        tBayes_normal= hurdle_normal_test(data1)\n",
    "    except:\n",
    "        tBayes_normal=0.50\n",
    "    if os.path.exists(resultFile):\n",
    "        with open(resultFile, 'rb') as fileName:\n",
    "          resultList = pickle.load(fileName)\n",
    "        resultList.append([tBayes_normal, t_mean, c_mean, p_value])\n",
    "        with open(resultFile, 'wb') as fileName:\n",
    "            pickle.dump(resultList,fileName)\n",
    "    else:\n",
    "        resultList=[[tBayes_normal, t_mean, c_mean, p_value]]\n",
    "        with open(resultFile, 'wb') as fileName:\n",
    "            pickle.dump(resultList,fileName)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce712b2-1572-4b09-b25e-9a74442af88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"All Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ff106-043b-4c4a-9214-f520d32210cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
